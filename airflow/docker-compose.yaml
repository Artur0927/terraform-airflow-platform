version: '3'

x-airflow-common:
  &airflow-common
  image: apache/airflow:2.9.2
  environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
    - AIRFLOW__CORE__FERNET_KEY=13F9yvz76PDDNsVM-b_pCs2vW8rPd1QVHb5_yXzaRbk=
    - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
    - AIRFLOW__CORE__LOAD_EXAMPLES=False
    - _PIP_ADDITIONAL_REQUIREMENTS=
    # UI Customization
    - AIRFLOW__WEBSERVER__INSTANCE_NAME=ðŸš€ Artur Data Platform
    - AIRFLOW__WEBSERVER__NAVBAR_COLOR=#00CDCD
    # Optimization for t3.small (Standard Mode)
    - AIRFLOW__WEBSERVER__WORKERS=2
    - AIRFLOW__CORE__PARALLELISM=2
    - AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL=30
  depends_on:
    - postgres
  user: "${AIRFLOW_UID:-50000}:0"
  env_file:
    - .env

services:
  postgres:
    image: postgres:15
    restart: always
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    restart: always
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 5
    volumes:                          # âœ… Ð²Ð°Ð¶Ð½Ð¾
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    restart: always
    volumes:                          # âœ… Ð²Ð°Ð¶Ð½Ð¾
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins

  airflow-init:
    <<: *airflow-common
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=13F9yvz76PDDNsVM-b_pCs2vW8rPd1QVHb5_yXzaRbk=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - _AIRFLOW_DB_MIGRATE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=airflow
      - _AIRFLOW_WWW_USER_PASSWORD=airflow
    entrypoint: /bin/bash
    command:
      - -c
      - |
        mkdir -p /sources/logs /sources/dags /sources/plugins
        chown -R "${AIRFLOW_UID}:0" /sources/{logs,dags,plugins}
        chown -R "${AIRFLOW_UID}:0" /sources/{logs,dags,plugins}
        exec /entrypoint airflow version && airflow db migrate && airflow users create --username airflow --password airflow --firstname Airflow --lastname Admin --role Admin --email admin@example.com && airflow connections add 'dwh_postgres' --conn-uri 'postgresql://airflow:airflow@postgres:5432/airflow' && airflow variables set s3_bucket_name airflow-data-platform-prod-bucket
    user: "0:0"
    volumes:
      - ./dags:/sources/dags
      - ./logs:/sources/logs
      - ./plugins:/sources/plugins

volumes:
  postgres_data:
